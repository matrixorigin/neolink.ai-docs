---
sidebar_position: 1
title: 模型介绍
sidebar_label: 模型介绍
---
Neolink.AI 的模型服务支持以下模型，其中部分模型需要申请开通。

| 厂商   | 模型名称                     | 描述                                                                                                    |
|--------|------------------------------|---------------------------------------------------------------------------------------------------------|
| Meta   | Meta-Llama-3-70B            | Llama3 是 Meta 最新发布的大语言模型，擅长语言的细微差别处理、上下文理解、代码生成、翻译等复杂任务。Meta-Llama-3-70B 版本拥有 700 亿参数，支持 8k 上下文，适用于对话问答和各种自然语言生成任务。 |
| Meta   | Meta-Llama-3-8B             | Llama3 是 Meta 最新发布的大语言模型，擅长语言的细微差别处理、上下文理解、代码生成、翻译等复杂任务。Meta-Llama-3-8B 版本拥有 80 亿参数，支持 8k 上下文，适用于对话问答和各种自然语言生成任务。 |
| Meta   | Meta-Llama-3.1-405B         | Meta 的 Llama 3.1 多语言大语言模型（LLM）集合，包含了预训练和指令微调的生成模型，大小分别为 80 亿、700 亿和 4050 亿参数（文本输入/文本输出）。Llama 3.1 405B指令微调的纯文本模型，4050 亿参数，针对多语言对话场景进行了优化，在常见的行业基准测试中，它们的表现优于许多可用的开源和闭源聊天模型。 |
| Mixtral | Mixtral-8x7B               | 由Mistral AI发布的首个高质量稀疏专家混合模型 (MOE)，模型由8个70亿参数专家模型组成，在多个基准测试中表现优于Llama-2-70B及GPT3.5，能够处理32K上下文，在代码生成任务中表现尤为优异。 |
| 百川智能 | Baichuan-13B-Chat           | Baichuan-13B 是由百川智能继Baichuan-7B 之后开发的包含130 亿参数的开源可商用的大规模语言模型，在权威的中文和英文benchmark 上均取得同尺寸最好的效果。 |
| 通义千问 | Qwen2-72B-Instruct         | Qwen2 是 Qwen 大型语言模型的最新系列。在 Qwen2 中，发布了一系列基础语言模型和指令调优语言模型，参数量从 0.5 亿到 720 亿不等，其中包括一个专家混合模型。本仓库包含指令调优的 72B Qwen2 模型。 |
| 通义千问 | Qwen2-7B-Instruct          | Qwen2 是 Qwen 大型语言模型系列的新版本。在 Qwen2 中，发布了一系列基础语言模型和指令调优语言模型，参数量从 0.5 亿到 720 亿不等，其中包括一个专家混合模型。此代码库包含指令调优的 7B Qwen2 模型。 |
| 通义千问 | Qwen1.5-14B-Chat          | Qwen1.5 是 Qwen2 的测试版，保持了作为仅解码器的变换模型架构，拥有 SwiGLU 激活函数、 RoPE 和多头注意力机制，拥有九种模型大小，增强了多语言和聊天模型能力，支持 32768 令牌的上下文长度，所有模型均启用了系统提示以实现角色扮演，代码支持 transformers 原生实现。 |
| 通义千问 | Qwen1.5-110B-Chat         | Qwen1.5 是 Qwen2 的测试版，保持了作为仅解码器的变换模型架构，拥有 SwiGLU 激活函数、 RoPE 和多头注意力机制，拥有九种模型大小，增强了多语言和聊天模型能力，支持 32768 令牌的上下文长度，所有模型均启用了系统提示以实现角色扮演，代码支持 transformers 原生实现。 |
| 通义千问 | gte-Qwen2-7B-instruct      | gte-Qwen2-7B-instruct 是 gte 嵌入系列的最新成员。该模型从 Qwen2-7B 大型语言模型出发，汲取了 Qwen1.5-7B 模型强大的自然语言处理能力。通过先进的嵌入训练技术的提升，该模型融合了多项关键改进。 |
| DeepSeek | DeepSeek-V2-Chat           | DeepSeek-V2 是一个强大的专家混合（MoE）语言模型，其特点是经济的训练和高效的推理。它总共包含 2360 亿个参数，其中每个标记激活了 210 亿个参数。与 DeepSeek 67B 相比，DeepSeek-V2 实现了更强的性能，同时节省了 42.5% 的训练成本，减少了 93.3% 的 KV 缓存，并将最大生成吞吐量提高了 5.76倍。 |
| Yi      | Yi-1.5-34B-Chat            | Yi-34B是由零一万物开发并开源的双语大语言模型，使用4K序列长度进行训练，在推理期间可扩展到32K；模型在多项评测中全球领跑，取得了多项 SOTA 国际最佳性能指标表现。 |
| 智谱AI  | ChatGLM3-6B-32K            | ChatGLM3 是智谱AI和清华大学 KEG 实验室联合发布的新一代对话预训练模型。ChatGLM3-6B 是 ChatGLM3 系列中的开源模型，在保留了前两代模型对话流畅、部署门槛低等众多优秀特性的基础上，ChatGLM3-6B 拥有更强大的基础模型，更完整的功能支持和更全面的开源序列。 |
| 智谱AI  | GLM-4-9B-Chat               | GLM-4-9B 是智谱 AI 推出的最新一代预训练模型 GLM-4 系列中的开源版本，能进行多轮对话，GLM-4-9B-Chat 还具备网页浏览、代码执行、自定义工具调用（Function Call）和长文本推理（支持最大 128K 上下文）等高级功能。 |
| BAAI    | BGE-M3                     | BGE M3-Embedding来自BAAI和中国科学技术大学，是BAAI开源的模型，它在多语言性（Multi-Linguality）、多功能性（Multi-Functionality）和多粒度性（Multi-Granularity）方面表现出色。M3-Embedding支持超过100种工作语言，支持8192长度的输入文本，同时支持密集检索（Dense Retrieval）、多向量检索（Multi-Vector Retrieval）和稀疏检索（Sparse Retrieval），通过这几种检索方式的组合，取得了良好的混合召回效果。 |