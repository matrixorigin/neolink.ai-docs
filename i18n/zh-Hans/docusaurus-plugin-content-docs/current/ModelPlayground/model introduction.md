---
sidebar_position: 1
title: 模型介绍
sidebar_label: 模型介绍
---

Neolink.AI 的模型服务支持以下模型，其中部分模型需要申请开通。

| 厂商     | 模型名称              | 描述                                                                                                                                                                                                                                                                                                                                                                                                           |
| -------- | --------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Meta     | Meta-Llama-3-70B      | Llama3 是 Meta 最新发布的大语言模型，擅长语言的细微差别处理、上下文理解、代码生成、翻译等复杂任务。Meta-Llama-3-70B 版本拥有 700 亿参数，支持 8k 上下文，适用于对话问答和各种自然语言生成任务。                                                                                                                                                                                                                |
| Meta     | Meta-Llama-3-8B       | Llama3 是 Meta 最新发布的大语言模型，擅长语言的细微差别处理、上下文理解、代码生成、翻译等复杂任务。Meta-Llama-3-8B 版本拥有 80 亿参数，支持 8k 上下文，适用于对话问答和各种自然语言生成任务。                                                                                                                                                                                                                  |
| Mixtral  | Mixtral-8x7B          | 由 Mistral AI 发布的首个高质量稀疏专家混合模型 (MOE)，模型由 8 个 70 亿参数专家模型组成，在多个基准测试中表现优于 Llama-2-70B 及 GPT3.5，能够处理 32K 上下文，在代码生成任务中表现尤为优异。                                                                                                                                                                                                                   |
| 百川智能 | Baichuan-13B-Chat     | Baichuan-13B 是由百川智能继 Baichuan-7B 之后开发的包含 130 亿参数的开源可商用的大规模语言模型，在权威的中文和英文 benchmark 上均取得同尺寸最好的效果。                                                                                                                                                                                                                                                         |
| 通义千问 | Qwen2-72B-Instruct    | Qwen2 是 Qwen 大型语言模型的最新系列。在 Qwen2 中，发布了一系列基础语言模型和指令调优语言模型，参数量从 0.5 亿到 720 亿不等，其中包括一个专家混合模型。本仓库包含指令调优的 72B Qwen2 模型。                                                                                                                                                                                                                   |
| 通义千问 | Qwen2-7B-Instruct     | Qwen2 是 Qwen 大型语言模型系列的新版本。在 Qwen2 中，发布了一系列基础语言模型和指令调优语言模型，参数量从 0.5 亿到 720 亿不等，其中包括一个专家混合模型。此代码库包含指令调优的 7B Qwen2 模型。                                                                                                                                                                                                                |
| 通义千问 | Qwen1.5-14B-Chat      | Qwen1.5 是 Qwen2 的测试版，保持了作为仅解码器的变换模型架构，拥有 SwiGLU 激活函数、 RoPE 和多头注意力机制，拥有九种模型大小，增强了多语言和聊天模型能力，支持 32768 令牌的上下文长度，所有模型均启用了系统提示以实现角色扮演，代码支持 transformers 原生实现。                                                                                                                                                 |
| 通义千问 | Qwen1.5-110B-Chat     | Qwen1.5 是 Qwen2 的测试版，保持了作为仅解码器的变换模型架构，拥有 SwiGLU 激活函数、 RoPE 和多头注意力机制，拥有九种模型大小，增强了多语言和聊天模型能力，支持 32768 令牌的上下文长度，所有模型均启用了系统提示以实现角色扮演，代码支持 transformers 原生实现。                                                                                                                                                 |
| 通义千问 | gte-Qwen2-7B-instruct | gte-Qwen2-7B-instruct 是 gte 嵌入系列的最新成员。该模型从 Qwen2-7B 大型语言模型出发，汲取了 Qwen1.5-7B 模型强大的自然语言处理能力。通过先进的嵌入训练技术的提升，该模型融合了多项关键改进。                                                                                                                                                                                                                    |
| DeepSeek | DeepSeek-V2-Chat      | DeepSeek-V2 是一个强大的专家混合（MoE）语言模型，其特点是经济的训练和高效的推理。它总共包含 2360 亿个参数，其中每个标记激活了 210 亿个参数。与 DeepSeek 67B 相比，DeepSeek-V2 实现了更强的性能，同时节省了 42.5% 的训练成本，减少了 93.3% 的 KV 缓存，并将最大生成吞吐量提高了 5.76 倍。                                                                                                                       |
| Yi       | Yi-1.5-34B-Chat       | Yi-34B 是由零一万物开发并开源的双语大语言模型，使用 4K 序列长度进行训练，在推理期间可扩展到 32K；模型在多项评测中全球领跑，取得了多项 SOTA 国际最佳性能指标表现。                                                                                                                                                                                                                                              |
| 智谱 AI  | ChatGLM3-6B-32K       | ChatGLM3 是智谱 AI 和清华大学 KEG 实验室联合发布的新一代对话预训练模型。ChatGLM3-6B 是 ChatGLM3 系列中的开源模型，在保留了前两代模型对话流畅、部署门槛低等众多优秀特性的基础上，ChatGLM3-6B 拥有更强大的基础模型，更完整的功能支持和更全面的开源序列。                                                                                                                                                         |
| 智谱 AI  | GLM-4-9B-Chat         | GLM-4-9B 是智谱 AI 推出的最新一代预训练模型 GLM-4 系列中的开源版本，能进行多轮对话，GLM-4-9B-Chat 还具备网页浏览、代码执行、自定义工具调用（Function Call）和长文本推理（支持最大 128K 上下文）等高级功能。                                                                                                                                                                                                    |
| BAAI     | BGE-M3                | BGE M3-Embedding 来自 BAAI 和中国科学技术大学，是 BAAI 开源的模型，它在多语言性（Multi-Linguality）、多功能性（Multi-Functionality）和多粒度性（Multi-Granularity）方面表现出色。M3-Embedding 支持超过 100 种工作语言，支持 8192 长度的输入文本，同时支持密集检索（Dense Retrieval）、多向量检索（Multi-Vector Retrieval）和稀疏检索（Sparse Retrieval），通过这几种检索方式的组合，取得了良好的混合召回效果。 |
